## 回归

1. 回归算法-线性回归分析

   * 线性回归：寻找一种能预测的趋势

   * 定义：线性回归通过一个或者多个**自变量**与**因变量**之间进行建模的回归分析。其中可以为**一个或多个自变量之间的线性组合（线性回归的一种）**

   * 一元线性回归：涉及到的变量只有一个

   * 多元线性回归：涉及到的变量两个或两个以上

   * 通用公式：$h(w) = w_0 + w_1x_1 + w_2x_2 + ... = w^Tx$

     其中w,x为矩阵：$w = \left(\begin{matrix}w_0 \\ w_1 \\ w2\end{matrix}\right)$,$x = \left(\begin{matrix}1 \\ x_1 \\ x2\end{matrix}\right)$

     ​

   * 线性关系：二维---直线，三维---平面

     * 二维线性关系模型：y = wx + b

   * 线性关系模型

     * 一个通过**属性的线性组合**来进行预测的函数：

       $f(x) = w_1x_1 + w_2x_2 + ... + w_dx_d + b$

       w为权重，b称为偏置项，可以理解为：$w_0*1$

   * 损失函数（误差大小）

     * yi为第i个训练样本的真实值

     * $h_w(x_i)$为第i个训练样本特征值组合预测函数

     * 总损失定义：

       $J(\theta) = (h_w(x_1) - y1)^2 + (h_w(x_2) - y_2)^2+...+ (h_w(x_m) - y_m)^2 \\ = \sum_{i=1}^m(h_w(x_i) - y_i)^2)$

       又称**最小二乘法**

     * 如何去求模型当中的W，使得损失最小？（**目的是找到最小损失对应的W值**）

       * 最小二乘之**正规方程**（不做要求）

         * 求解：$w = (X^TX)^{-1}X^Ty$

           X为特征值矩阵，y为目标值矩阵

         * 缺点：**当特征过于复杂，求解速度太慢**

       * 最小二乘之**梯度下降**（理解过程）

         * 我们以单变量中的w0，w1为例子：

           $w1 := -w1 - \alpha\frac{\delta cost(w0 + w1x1)}{\delta w1}$

           $w0 := -w0 - \alpha\frac{\delta cost(w0 + w1x1)}{\delta w1}$

           **$\alpha$为学习速率，需要手动指定**，$\alpha\frac{\delta cost(w0 + w1x1)}{\delta w1}$ 表示**方向**

         * 理解：沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值

         * 使用：**面对训练数据规模十分庞大的任务**

     * sklearn API

       * 正规方程：sklearn.linear_model.LinearRegression()
         * 普通最小二乘线性回归
         * coef_：回归系数(w)
       * 梯度下降：sklearn.linear_model.SGDRegressor
         * 通过使用SGD最小化线性模型
         * coef_：回归系数

     * 正规方程与梯度下降的对比

       | 梯度下降                    | 正规方程                                                     |
       | --------------------------- | ------------------------------------------------------------ |
       | 需要选择学习率$\alpha$      | 不需要                                                       |
       | 需要多次迭代                | 一次运算得出                                                 |
       | 当特征数量n大时也能较好适用 | 需要计算$(X^TX)^{-1}$ , 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当n小于100,000时还是可以接受的 |
       | 适用于各类型的模型          | 只适用于线性模型，不适合逻辑回归模型等其它模型               |

       * 特定：线性回归器时最为简单、易用的回归模型。从某种程度上限制了适用，尽管如此，在不知道特征之间关系的前提下，我们仍然适用线性回归器作为大多数系统的首选

         小规模数据：LinearRegression（不能解决**拟合问题**）以及其它

         大规模数据：SGDRegressor

2. 线性回归实例

   波士顿房价数据集分析流程

   1. 波士顿地区房价数据获取
   2. 波士顿地区房价数据分割
   3. **训练与测试数据标准化处理**
   4. 使用最简单的线性回归模型LinearRegression和梯度下降估计SGDGressor对房价进行预测

3. 回归性能评估

   - （均方误差（Mean Squared Error）MSE）评价机制：

     $MSE = \frac{1}{m}\sum_{i=1}^m(y^i - \overline{y})^2$

     注：$y^i$为预测值，$\overline{y}$为真实值

   - LAPI

   - sklearn.metrics.mean_squared_error(y_true, y_pred)

     - 均方误差回归损失
     - y_true：真实值
     - y_pred：预测值
     - return：浮点数结果

     注：真实值，预测值为**标准化之前的值**

   - 问题：训练数据训练的很好啊，误差也不大，为什么在测试集上面有问题呢？

     - **过拟合**：一个假设**在训练数据上能够获得比其他假设更好的你和**，但是在**训练数据外的数据集却不能很好地拟合数据**，此时认为这个假设出现了过拟合现象。（**模型过于复杂**）
     - **欠拟合**：一个假设**在训练数据上不能获得更好的拟合**，但是在**训练数据外的数据集上也不能很好地拟合数据**，此时认为这个假设出现了欠拟合现象。（**模型过于简单**）
     - 对线性模型进行训练学习会变成复杂模型
       - 模型复杂的原因：数据的特征和目标值之间的关系不仅仅是线性关系

   - **欠拟合原因以及解决办法**

     - 原因
       - 学习到数据的特征过少
     - 解决办法
       - 增加数据的特征数量

   - **过拟合原因以及解决办法**

     - 原因
       - 原始特征过多，存在一些嘈杂特征，模型过于复杂是因为没模型尝试去兼容各个测试数据点
     - 解决办法：
       - 进行特征选择，消除关联性大的特征（很难做）
       - 交叉验证（让所有数据都有过训练）
       - **正则化（了解）**

   - 根据结果现象判断：欠拟合，过拟合

     - 交叉验证：训练集结果、测试集结果

   - L2正则化

     - 作用：可以**使得W的每个元素都很小，都接近于0**

     - 优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象

       **LinearRegression**：容易出现过拟合，为了把训练数据表现更好。

       **Ridge岭回归**：带有正则化的线性回归，解决过拟合

   - 岭回归API

     - sklearn.linear_model.Ridge(alpha=1.0)

       - 具有L2正则化的线性最小二乘法

         ​

       - alpha：正则化力度  **$\lambda$** （0~1 1~10）

         - 力度越大，越接近于0

       - coef_：回归系数

     - 线性回归LinearRegression与Ridge对比

       - 岭回归：回归得到**回归系数更符合实际，更可靠**。另外，能让估计参数的波动范围变小，变的更稳定。在存在病态数据偏多的研究中有较大的实用价值

   - sklearn模型的保存和加载

     - from sklearn.externals import joblib

     - 保存：joblib.dump(rf, 'test.pkl')

     - 加载：estimator = joblib.load('test.pkl')

       **注：文件格式pkl**

       ​

4. 分类算法-逻辑回归

   * 应用场景（二分类问题）

     * 广告点击率
     * 是否为垃圾邮件
     * 是否患病
     * 金融诈骗
     * 虚假账号

   * 逻辑回归

     * 输入：$Z(x) = w_0 + w_1x_1 + w_2x_2 + ... = w^Tx$

       （单个样本）

     * sigmoid函数（重点，需要记住）

       * y轴：[0~1]

       * 与x轴的交点：默认0.5

       * 公式

         $h_{\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}}$

         $g(z) = \frac{1}{1 + e^{-z}}$

         输出：**[0,1]区间的概率值，默认0.5作为阈值**

         注：g(z)为sigmoid函数

     * 逻辑回归的损失函数、优化（了解）

       与线性回归原理相同，但由于是分类问题，**损失函数**不一样，只能通过梯度下降求解

       * 对数似然损失函数：

         $cost(h_\theta(x),y) = \begin{cases}-log(h_\theta(x)) & if \,y=1 \\ -log(1 - h_\theta(x)) & if\,y=0\end{cases}$

         完整的损失函数：

         $cost(h_\theta(x),y) = \sum_{i=1}^m-y_ilog(h_\theta(x)) - (1-y_i)log(1-h_\theta(x))$

         **cost损失的值越小，那么预测的类别准确度更高** 

     * 损失函数

       * 均方误差：不存在多个局部最低点，只有一个最小值
       * 对数似然损失：存在多个局部最小值（目前解决不了）
         * 尽管没有全局最低点，但是效果都是不错的
         * 改善方案（sklearn接口中已自动实现）
         * 1. 多次随机初始化，多次比较最小值结果
           2. 求解过程中，调整学习率

     * sklearn逻辑回归API

       sklearn.linear_model.LogisticRegression(penalty='l2', C=1.0)

       * Logistic回归分类器
       * coef_：回归系数 

        

5. 逻辑回归实例

   良 / 恶性乳腺癌肿数据

   原始数据的下载地址：https://archive.ics.uci.edu/ml/machine-learning-databases

   * 数据描述

   1. 699条样本，共11列数据，第一列用于检索的id，后9列分别是与肿瘤相关的医学特征，最后一列表示肿瘤类型的数值
   2. 包含16个缺失值，用“？”标出 

   * 流程

   * 1. 网上获取数据（工具pandas）

        pd.read_csv(’’,names=column_names)

        * column_names：指定类别名字,['Sample code number','Clump

        Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']

        * return:数据

        ​

        * replace(to_replace=’’,value=)：返回数据
        * dropna():返回数据

     2. 数据缺失值处理、标准化

     3. LogisticRegression估计器流程

   * LogisticRegression总结

     * 应用：（二分类问题）广告点击率预测、电商购物搭配推荐、是否患病、金融诈骗、是否为虚假账号
     * 优点：适合需要得到一个分类预测概率的场景，简单，速度快
     * 缺点：不好处理多分类，当特征空间很大时，逻辑回归的性能不是很好（看硬件能力）

     |            | 逻辑回归                                       | 朴素贝叶斯                                                   |
     | ---------- | ---------------------------------------------- | ------------------------------------------------------------ |
     | 解决问题   | 二分类                                         | 多分类问题                                                   |
     | 应用场景   | 癌症，二分类需要概率                           | 文本分类                                                     |
     | 参数       | 正则化力度                                     | 无                                                           |
     | 得出的结果 | 有概率解释                                     | 有概率解释                                                   |
     | 模型类别   | 判别模型（k-近邻，决策树，随机森林，神经网络） | 生成模型-存在先验概率P(c)（从历史信息中总结出概率信息）（隐马尔可夫模型） |

6. 聚类算法-kmeans

   k：把数据划分成多少个类别

   聚类一般做在分类之前，分好类后对数据进行预测

   * 假设k = 3

     * 1. 随即在数据当中抽取三个样本，当作三个类别的中心（k1,k2,k3）
       2. 计算其余的点分别到这三个中心点的距离，每一个样本有三个距离(a,b,c)，从中选出距离最近的一个点作为自己的标记，形成三个族群
       3. 分别计算这三个族群的平均值，把三个平均值与之前的三个旧中心点进行比较
          * 如果相同：结束聚类
          * 如果不相同：把这三个平均值当作新的中心点，重复第二步

     * k-means步骤

       * 1. **随机**设置K个特征空间内的点作为初始的聚类中心
         2. 对于其他每个点计算到K个中心的距离，**未知的点选择最近的一个聚类中心点作为标记类别**
         3. 接着对这标记的聚类中心，重新计算出每个聚类的新中心点（平均值）
         4. 如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程

       * k-means API

       * sklearn.cluster.KMeans(n_clusters=8,init='k-means++')

         * k-means聚类
         * n_cluster：开始的聚类中心数量
         * init：初始化方法，默认为'k-means++'
         * labels_：默认标记的类型，可以和真实值比较（不是值比较）

       * 聚类评估标准

         * 轮廓系数（内部距离最小化，外部距离最大化）

           计算公式：$SC_i = \frac{b_i-a_i}{max(b_i\,a_i)}$

           注：对于每个点i 为已聚类数据中的样本，**bi 为i 到其它族群**的所有样本的平均距离，**ai  为i 到本身簇**的距离平均值

           最终计算出所有的样本点的轮廓系数平均值

           * 如果SCi小于0，说明ai的平均距离大于最近的其他簇。聚类效果不好
           * 如果SCi越大，说明ai的平均距离小于最近的其他簇。聚类效果好
           * 轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优，一般很难超过0.7

           ​	bi >> ai：1（完美）

           ​	ai >> bi：-1（最差）

         * API

         * sklearn.metrics.silhouette_score(X, labels)

           * 计算所有样本的平均轮廓系数
           * X：特征值
           * labels：被聚类标记的目标值

     * Kmeans总结

       * 特点：采用迭代式算法，直观易懂并且非常实用
       * 缺点：
         * 容易收敛到**局部最优解(多次聚类)**
         * 需要预先设定簇的数量**(k-means++解决)** 

7. k-means实例

   * k-means对Instacart Market用户聚类
     * 1. 降维之后的数据
       2. k-means聚类
       3. 聚类结果显示