## 机器学习算法基础

1. sklearn数据集与估计器

   1. 数据集划分

      * 训练集：用于训练，**建立模型** ，一般占比75%
      * 测试集：在模型检验时使用，用于**评估模型**是否有效，一般占比25%

   2. sklearn数据集接口介绍

      **sklean数据集API** 

      * sklearn.datasets

      		加载获取流行数据集

      		datasets.load_*()：获取**小规模数据集**，数据包含在datasets里

      * dataset.fetch_*(data_home=None)

      		获取**大规模数据集**，需要从网络上下载，函数的第一个参数是data_home，标识数据集下载的目录，默认是~/scikit_learn_data/

      * 获取数据集返回的类型

      		load* 和fetch*返回数据类型datasets.base.Bunch(**字典格式**)

      		data：特征数据数组，是[n_samples * n_features]的二维numpy.ndarray数组
   		
      		target：标签数组，是n_samples的一维numpy.ndarray数组
   		
      		DESCR：数据描述
   		
      		feature_name：特征名，**新闻数据，手写数据，回归数据集没有** 
   		
      		target_names：标签名

      **数据集划分API**  

      - sklearn.model_selection.train_test_split(*arrays, **options)
        - x：数据集的特征值
        - y：数据集的标签值
        - test_size：测试集的大小，一般为float
        - random_state：随机数种子，不同的种子会造成不同的随机采样结果，相同的种子采样结果相同
        - return：训练集特征值，测试集特征值，训练标签，测试标签（默认随机取）

   3. sklearn分类数据集

      **用于分类的大数据集** 

      - sklearn.datasets.fetch_20newgroups(data_home=None,subset='train')
        - subset:'train'或者'test','all',可选，选择要加载的数据集。训练集的“训练”，测试集的“测试”，两者的“全部”
      - datasets.clear_data_home(data_home=None)
        - 清除目录下的数据

   4. sklearn回归数据集

      **回归数据集API**

      * sklearn.datasets.load_boston()
        * 加载并返回波士顿房价数据集
      * sklearn.datasets.load_diabetes()
        * 加载并返回糖尿病数据集

   5. 转换器与估值器

      转换器

      * fit_transform()：输入数据直接转换
      * fit()：输入数据，不转换
      * transtorm()：进行数据的转换

      估计器

      * 在sklearn中，估计器(estimator)是一个重要的角色，**是一类实现了算法的API**
      * 用于分类的估计器：
        * sklearn.neighbors	k-近邻算法
        * sklearn.naive_bayes  贝叶斯
        * sklearn.linear_model.LogisticRegression 逻辑回归
        * sklearn.tree               决策树与随机森林
      * 用于回归的估计器：
        * sklearn.linear_model.LinearRegression   线性回归
        * sklearn.linear_model.Ridge   岭回归

      应用步骤

      * 调用fit传入训练集特征特征值即目标值: fit(x_train, y_train)
      * 输入测试集数据获取预测结果: y_predict = predict(x_test)
      * 得到预测的准确率: score(x_test, y_test)

2. 分类算法-k近邻算法（KNN）

   * 定义：如果一个样本在特征空间中的**k个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别**，则该样本也属于这个类别
   * 来源：KNN算法最早是由Cover和Hart提出的一种分类算法
   * 思想：相似的样本，特征之间的值应该都是相近的
   * 计算距离公式
     * 两个样本的距离可以通过如下公式计算，又叫**欧式距离**
     * 比如说，a(a1,a2,a3),b(b1,b2,b3)
       * $\sqrt{(a1-a2)^2 + (a2-b2)^3 + (a3-b3)^2}$
   * sklearn k-近邻算法API
   * sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
     * n_neighbors: int,可选（默认=5），k_neighbors查询默认使用的邻居数
     * algorithm：{'auto', 'ball_tree', 'kd_tree', 'brute'}, 可选用于计算最近邻居的算法
       * ball_tree：使用BallTree
       * kd_tree：使用KDTree
       * auto：尝试根据传递给fit方法的值来决定最合适的算法。（不同实现方式影响效率）

3. k-近邻算法实例

   * K-近邻预测用户签到位置
     * 下载数据集：<https://www.kaggle.com/c/facebook-v-predicting-check-ins/data>
     * 特征值：x,y坐标，定位准确度，时间戳
     * 目标值：入住的位置id
   * 步骤
     * 处理数据
     * 特征工程（标准化）
     * 进行算法流程
   * 总结
     * 问题
       * k值取多大？有什么影响？
         1. k值取很小：容易受异常点影响
         2. k值取很大：容易受K值数量（类别）波动
       * 性能问题
     * 优点：简单，易于理解，易于实现，**无需估计参数，无需训练**
     * 缺点：
     * 1. 懒惰算法，对测试样本分类时的计算量大，内存开销大
       2. 必须指定K值，K值选择不当则分类精度不能保证
     * 使用场景：小数据场景，几千~几万样本，具体场景具体业务去测试

4. 分类模型的评估

   * estimator.score()

     * 一般最常见使用的时**准确率**，即预测结果正确的百分比

   * 混淆矩阵

     * 在分类任务下，预测结果（Predicted Condition）与正确标记（True Condition）之间存在四种不同的组合，构成混淆矩阵（适用于多分类）

     * | 真实结果\预测结果 | 正例     | 假例     |
       | :---------------- | -------- | -------- |
       | 正例              | 真正例TP | 伪反例FN |
       | 假例              | 伪正例FP | 真反例TN |

     * 精确率（Precision）：**预测结果为正例**样本中真实为正例的比例（查得准）（TP/(TP+FP）

     * **召回率（Recall）**：**真实为正例的样本**中预测结果为正例的比例（查的全，对正样本的区分能力）（TP/(TP+FN)）

     * 其他分类标准：**F1-score**，反映了模型的**稳健性**

       $F1 = \frac{2TP}{2TP+FN+FP} = \frac{2\. Precision\.Recall}{Precision+Recall}$

   * 分类模型评估API

   * sklearn.metrics.classification_report(y_true, y_pred, target_name=None)

     * y_true：真实目标值
     * y_pred：估计器预测目标值
     * target_names：目标类别名称
     * return：每个类别精确率与召回率

5. 分类算法-朴素贝叶斯算法

   概率基础

   * 概率定义为**一件事情发生的可能性**
     * 扔出一个硬币，结果头像朝上的概率 1/2
     * 某天是晴天，需要根据历史数据评估
   * 联合概率：包含多个条件，且所有条件同时成立的概率
     * 记作：**P(A,B)**
     * P(A,B)=P(A)P(B)
   * 条件概率：就是事件A在另外一件事件B已经发生条件下的发生概率
     * 记作：P(A|B)
     * 特性：P(A1,A2|B) = P(A1|B)P(A2|B)
     * **注意：此条件概率的成立，是由于A1,A2相互独立的结果**

   朴素贝叶斯介绍

   * 贝叶斯公式
     * $P(C|W) = \frac{P(W|C)P(C)}{P(W)}$
       * 注：w为给定文档的特征值（频数统计，预测文档提供），c为文档类别
     * 公式可以理解为：
       * $P(C|F1,F2,...) = \frac{P(F1,F2,...|C)P(C)}{P(F1,F2,...)}$
       * 其中c可以是不同类别
       * P(C)：每个文档类别的概率（某文档类别数 / 总文档数量）
       * P(W|C)：给定类别下特征（**被预测文档中出现的词**）的概率
         * 计算方法：P(F1|C) = Ni/N   (**根据训练文档数据计算**)
         * Ni为该F1词在C类别所有文档中出现的 次数
         * N为所属类别C下的文档所有词出现的次数和
       * P(F1,F2,...)：预测文档中每个词的概率
     * 拉普拉斯平滑
       * 问题：如果词频列表里面有很多出现次数为0的特征值，很可能计算结果都为0
       * 解决方法：**拉普拉斯平滑系数$\alpha$**
         * $P(F1|C) = \frac{Ni + \alpha}{N + \alpha m}$
         * a为指定的系数，一般为1，m为训练文档中统计出的特征词个数
     * sklearn朴素贝叶斯实现API
     * sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
       * 朴素贝叶斯分类
       * alpha：拉普拉斯平滑系数
   * 总结
     * 训练集误差大的情况下，结果肯定不好
     * 不需要调参
     * 优点
       * 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率
       * **对缺失数据不太敏感**，算法也比较简单，常用于文本分类
       * **分类准确度高，速度快**
     * 缺点
       * 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好

6. 朴素贝叶斯算法实例

7. 模型的选择与调优

   1. 交叉验证

      * 目的：为了让被评估的模型更加准确可信

      * 验证过程：将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成5份，其中一份作为验证集。然后经过5次（组）的测试，每次都更换不同的验证集。即得到5组模型的结果，取平均值作为最终结果。又称**5折交叉验证**。

        | 分组 |        |        |        |        |        |
        | ---- | ------ | ------ | ------ | ------ | ------ |
        | 组1  | 验证集 | 训练集 | 训练集 | 训练集 | 训练集 |
        | 组2  |        | 验证集 |        |        |        |
        | 组3  |        |        | 验证集 |        |        |
        | 组4  |        |        |        | 验证集 |        |
        | 组5  |        |        |        |        | 验证集 |

   2. 网络搜索（ 超参数搜索）

      通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的k值），这种叫**超参数**。但是手动过程繁杂，所以需要对模型预设集中超参数组合。**每组超参数都采用交叉验证来进行评估**。一般采用10折交叉验证，最后选出最优参数组合建立模型

      | K值  | K=3   | K=5   | K=7   |
      | ---- | ----- | ----- | ----- |
      | 模型 | 模型1 | 模型2 | 模型3 |

      网格搜索API

      sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)

      * 对估计器的指定参数值进行详尽搜索
      * estimator：估计器对象
      * param_grid：估计器参数(dict){"n_neighbors":[1,3,5]}
      * cv：指定几折交叉验证
      * fit：输入训练数据
      * score：准确率
      * 结果分析：
        * best_score_：在交叉验证中验证的最好结果
        * best_estimator_：最好的参数模型
        * cv_result_：每次交叉验证后的测试集准确率结果和训练集准确率结果

8. 决策树与随机森林

   1. 认识决策树

      决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法

   2. 信息论基础-银行贷款分析

      * 信息论

        * 信息论的创始人，香农是密歇根大学学士，麻省理工学院博士
        * 1948年，香农发表了划时代的论文 —— 通信的数学原理，奠定了现代信息论的基础
        * 信息的单位：比特

      * 信息熵：

        * “谁是世界杯冠军”：一共32支球队，需要猜测谁是世界冠军
        * “谁是世界冠军”的信息量应该比5比特少。香农支出，它的准确信息量应该是

        $H = -(p1logp1 + p2logp2 + ... + p32logp32)$

        * H的专业术语称之为信息熵，单位为比特。

        * 公式：

          $H(X) = \sum_{x\epsilon{X}} P(x)logP(x) $

        * 当这32支球队夺冠的几率相同时，对应的信息熵等于5比特

      * **信息和消除不确定性是相联系的**

      * 决策树的划分依据之一 —— 信息增益

        * 信息增益：当得知一个特征条件之后，减少的信息熵大小

        * 特征A对训练数据集D的信息增益g(D,A)，定位为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：

          $g(D,A) = H(D) - H(D|A)$

          注：信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度

      * 银行贷款案例：

        * 特征：是否有房，是否有工作，信贷情况，年龄

        * | ID   | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |
          | ---- | ---- | ------ | ------------ | -------- | ---- |
          | 1    | 青年 | 否     | 否           | 一般     | 否   |
          | 2    | 青年 | 否     | 否           | 好       | 否   |
          | 3    | 青年 | 是     | 否           | 好       | 是   |
          | 4    | 青年 | 是     | 是           | 一般     | 是   |
          | 5    | 青年 | 否     | 否           | 一般     | 否   |
          | 6    | 中年 | 否     | 否           | 一般     | 否   |
          | 7    | 中年 | 否     | 否           | 好       | 否   |
          | 8    | 中年 | 是     | 是           | 好       | 是   |
          | 9    | 中年 | 否     | 是           | 非常好   | 是   |
          | 10   | 中年 | 否     | 是           | 非常好   | 是   |
          | 11   | 老年 | 否     | 是           | 非常好   | 是   |
          | 12   | 老年 | 否     | 是           | 好       | 是   |
          | 13   | 老年 | 是     | 否           | 好       | 是   |
          | 14   | 老年 | 是     | 否           | 非常好   | 是   |
          | 15   | 老年 | 否     | 否           | 一般     | 否   |

        * 信息熵的计算：

          $H(D) = - \sum_{k=1}^K\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}$

        * 条件熵的计算：

          $H(D|A) = \sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log\frac{|K_{ik}|}{|D_i|}$

          注：$C_k$表示属于某个类别的样本数

        * 根据上述公式，我们可以根据前面是否通过贷款申请的例子来通过计算得出我们的决策特征顺序。

          * 总的信息熵

            H(D) = -(9/15)log(9/15) - (6/15)log(6/15) = 0.971

          * 我们让A1,A2,A3,A4分别表示年龄、有工作、有自己的房子和信贷情况4个特征，则计算出年龄的信息增益为：

            年龄：g(D,A1) = H(D) - [(5/15)H(D1) + (5/15)H(D2) + (5/15)H(D3)]

            ​		H(D1) = -(2/5)log(2/5) - (3/5)log(3/5)

            ​		H(D2) = -(3/5)log(3/5) - (2/5)log(2/5)

            ​		H(D3) = -(4/5)log(4/5) - (1/5)log(1/5)

            同理其他的也可以计算出来，g(D,A2) = 0.324, g(D,A3) = 0.420,g(D,A4) = 0.363, 相比较来说其中特征A3（有自己的房子）的信息增益最大，所以我们选择特征A3为最优特征

        * 常见决策树使用的算法

          * ID3：信息增益 最大的准则
          * C4.5：信息增益比 最大的准则
          * CART
            * 回归树：平方误差 最小
            * 分类树：基尼系数（划分更加仔细） 最小的准则 在sklearn中可以选择划分的默认原则

   3. 决策树的生成

      - sklearn决策树API

      - class sklearn.tree.DecisionTreeClassfier(criterion='gini', max_depth=None, random_state=None)

        - 决策树分类器

        - criterion：默认是'gini'系数，也可以选择信息增益的熵'entropy'

        - max_depth: 树的深度大小

        - random_state: 随机数种子

          ​

        - method:

        - decision_path: 返回决策树的路径

      - 决策树的结构、本地保存

        - sklearn.tree.export_graphviz()该函数能够到处DOT格式

          tree.export_graphviz(estimator, out_file='tree.dot', feature_names=[","])

          ​	estimator：估计器

          ​	out_file：输出文件名

          ​	feature_names：根节点

        - 工具：能够将dot文件转换为pdf、png

          - 方式1

            安装graphviz

            ubuntu: sudo apt_get install graphviz

            Mac: brew install graphviz

            运行命令

            dot -Tpng tree.dot -o tree.png

          - 方式2（没成功运行）

            通过在线网站查看：<http://webgraphviz.com/>

   4. 泰坦尼克号乘客生存分类

      在泰坦尼克号和titanic2数据帧描述泰坦尼克号上的个别乘客的生存状态。这里使用的数据集是由各种研究人员开始的。其中包括许多研究人员创建的旅客名单，由Michael A. Findlay编辑。我们提取的数据集中的特征是票的类别，存活，乘坐班，年龄，登陆，home.dest，房间，票，船和性别。

      **乘坐班是指乘客班（1，2，3），是社会经济基层的代表**

      **其中age数据存在缺失**

      数据：<http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt>

      具体执行过程查阅代码

   5. 决策树的优缺点以及改进

      * 优点
        * 简单的理解和解释，树木可视化
        * 需要很少的数据准备，其它技术通常需要数据归一化
      * 缺点
        * 决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为过拟合
      * 改进
        * 减枝cart算法（**决策树API当中已经实现，随机森林参数调优有相关介绍**）
        * **随机森林**

      注：企业重要决策，由于决策树很好的分析能力，在决策过程应用较多

   6. 什么是随机森林（集成学习方法）

      集成学习方法

      * 集成学习通过建立几个模型组合的方式来解决单一预测问题。它的工作原理是**生成多个分类器/模型**，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的作出预测

      随机森林

      * 在机器学习中，**随机森林**是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

        例如，如果你训练了5个树，其中有4个树的结果是True，1个树的结果是False，那么最终结果会是True

   7. 随机森林的过程、优势

      过程

      ​	假设前提条件：N个样本，M个特征

      * 单个树建立过程：
        * 随机在N个样本中选择一个样本，重复N次（样本有可能重复）
        * 随机在M个特征当中选出m个特征（m <= M）

      依次进行建立其它树

      特点：

      * 建立10棵决策树，样本，特征大多不一样
      * 随机有放回的抽样（bootstrap）

      随机森林API

      * class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, bootstrap=True, random_state=None)

        * 随机森林分类器
        * n_estimators：integer, optional(default=10) 随机森林的树木数量

        经验值（120，200，300，500，800，1200）

        * criteria：string, 可选(default="gini")分割特征的测量方法
        * max_depth：integer或None,可选（默认=无）树的最大深度

        经验值（5，8，15，25，30）

        * max_features="auto"，每个决策树的最大特征数量
        * - if "auto", then 'max_features=sqrt(n_feature)'
          - if "sqrt", then 'max_features=sqrt(n_feature)'(same as "auto")
          - if "log2", then 'max_features=log2(n_feature)'
          - if None, then 'max_features=n_feature'
        * bootstrap：boolean, optional(default=True) 是否在构建树时使用放回抽样

      * 随机森林的优点

        * 在当前所有算法中，具有极好的准确率
        * 能够有效地运行在大数据集上
        * 能够处理具有高维特征的输入样本，而且不需要降维
        * 能够评估各个特征在分类问题上的重要性

   8. 泰坦尼克号乘客生存分类分析