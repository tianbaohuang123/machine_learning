## 机器学习算法基础

1. sklearn数据集与估计器

   1. 数据集划分

      * 训练集：用于训练，**建立模型** ，一般占比75%
      * 测试集：在模型检验时使用，用于**评估模型**是否有效，一般占比25%

   2. sklearn数据集接口介绍

      **sklean数据集API** 

      * sklearn.datasets

      ​	加载获取流行数据集

      ​	datasets.load_*()：获取**小规模数据集**，数据包含在datasets里

      * dataset.fetch_*(data_home=None)

      ​	获取**大规模数据集**，需要从网络上下载，函数的第一个参数是data_home，标识数据集下载的目录，默认是~/scikit_learn_data/

      * 获取数据集返回的类型

      ​	load* 和fetch*返回数据类型datasets.base.Bunch(**字典格式**)

      ​	data：特征数据数组，是[n_samples * n_features]的二维numpy.ndarray数组

      ​	target：标签数组，是n_samples的一维numpy.ndarray数组

      ​	DESCR：数据描述

      ​	feature_name：特征名，**新闻数据，手写数据，回归数据集没有** 

      ​	target_names：标签名

      **数据集划分API**  

      - sklearn.model_selection.train_test_split(*arrays, **options)
        - x：数据集的特征值
        - y：数据集的标签值
        - test_size：测试集的大小，一般为float
        - random_state：随机数种子，不同的种子会造成不同的随机采样结果，相同的种子采样结果相同
        - return：训练集特征值，测试集特征值，训练标签，测试标签（默认随机取）

   3. sklearn分类数据集

      **用于分类的大数据集** 

      - sklearn.datasets.fetch_20newgroups(data_home=None,subset='train')
        - subset:'train'或者'test','all',可选，选择要加载的数据集。训练集的“训练”，测试集的“测试”，两者的“全部”
      - datasets.clear_data_home(data_home=None)
        - 清除目录下的数据

   4. sklearn回归数据集

      **回归数据集API**

      * sklearn.datasets.load_boston()
        * 加载并返回波士顿房价数据集
      * sklearn.datasets.load_diabetes()
        * 加载并返回糖尿病数据集

   5. 转换器与估值器

      转换器

      * fit_transform()：输入数据直接转换
      * fit()：输入数据，不转换
      * transtorm()：进行数据的转换

      估计器

      * 在sklearn中，估计器(estimator)是一个重要的角色，**是一类实现了算法的API**
      * 用于分类的估计器：
        * sklearn.neighbors	k-近邻算法
        * sklearn.naive_bayes  贝叶斯
        * sklearn.linear_model.LogisticRegression 逻辑回归
        * sklearn.tree               决策树与随机森林
      * 用于回归的估计器：
        * sklearn.linear_model.LinearRegression   线性回归
        * sklearn.linear_model.Ridge   岭回归

      应用步骤

      * 调用fit传入训练集特征特征值即目标值: fit(x_train, y_train)
      * 输入测试集数据获取预测结果: y_predict = predict(x_test)
      * 得到预测的准确率: score(x_test, y_test)

2. 分类算法-k近邻算法（KNN）

   * 定义：如果一个样本在特征空间中的**k个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别**，则该样本也属于这个类别
   * 来源：KNN算法最早是由Cover和Hart提出的一种分类算法
   * 思想：相似的样本，特征之间的值应该都是相近的
   * 计算距离公式
     * 两个样本的距离可以通过如下公式计算，又叫**欧式距离**
     * 比如说，a(a1,a2,a3),b(b1,b2,b3)
       * $\sqrt{(a1-a2)^2 + (a2-b2)^3 + (a3-b3)^2}$
   * sklearn k-近邻算法API
   * sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
     * n_neighbors: int,可选（默认=5），k_neighbors查询默认使用的邻居数
     * algorithm：{'auto', 'ball_tree', 'kd_tree', 'brute'}, 可选用于计算最近邻居的算法
       * ball_tree：使用BallTree
       * kd_tree：使用KDTree
       * auto：尝试根据传递给fit方法的值来决定最合适的算法。（不同实现方式影响效率）

3. k-近邻算法实例

   * K-近邻预测用户签到位置
     * 下载数据集：<https://www.kaggle.com/c/facebook-v-predicting-check-ins/data>
     * 特征值：x,y坐标，定位准确度，时间戳
     * 目标值：入住的位置id
   * 步骤
     * 处理数据
     * 特征工程（标准化）
     * 进行算法流程
   * 总结
     * 问题
       * k值取多大？有什么影响？
         1. k值取很小：容易受异常点影响
         2. k值取很大：容易受K值数量（类别）波动
       * 性能问题
     * 优点：简单，易于理解，易于实现，**无需估计参数，无需训练**
     * 缺点：
     * 1. 懒惰算法，对测试样本分类时的计算量大，内存开销大
       2. 必须指定K值，K值选择不当则分类精度不能保证
     * 使用场景：小数据场景，几千~几万样本，具体场景具体业务去测试

4. 分类模型的评估

5. 分类算法-朴素贝叶斯算法

   概率基础

   * 概率定义为**一件事情发生的可能性**
     * 扔出一个硬币，结果头像朝上的概率 1/2
     * 某天是晴天，需要根据历史数据评估
   * 联合概率：包含多个条件，且所有条件同时成立的概率
     * 记作：**P(A,B)**
     * P(A,B)=P(A)P(B)
   * 条件概率：就是事件A在另外一件事件B已经发生条件下的发生概率
     * 记作：P(A|B)
     * 特性：P(A1,A2|B) = P(A1|B)P(A2|B)
     * **注意：此条件概率的成立，是由于A1,A2相互独立的结果**

   朴素贝叶斯介绍

   * 贝叶斯公式
     * $P(C|W) = \frac{P(W|C)P(C)}{P(W)}$
       * 注：w为给定文档的特征值（频数统计，预测文档提供），c为文档类别
     * 公式可以理解为：
       * $P(C|F1,F2,...) = \frac{P(F1,F2,...|C)P(C)}{P(F1,F2,...)}$
       * 其中c可以是不同类别
       * P(C)：每个文档类别的概率（某文档类别数 / 总文档数量）
       * P(W|C)：给定类别下特征（**被预测文档中出现的词**）的概率
         * 计算方法：P(F1|C) = Ni/N   (**根据训练文档数据计算**)
         * Ni为该F1词在C类别所有文档中出现的 次数
         * N为所属类别C下的文档所有词出现的次数和
       * P(F1,F2,...)：预测文档中每个词的概率
     * 拉普拉斯平滑
       * 问题：如果词频列表里面有很多出现次数为0的特征值，很可能计算结果都为0
       * 解决方法：**拉普拉斯平滑系数$\alpha$**
         * $P(F1|C) = \frac{Ni + \alpha}{N + \alpha m}$
         * a为指定的系数，一般为1，m为训练文档中统计出的特征词个数
     * sklearn朴素贝叶斯实现API
     * sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
       * 朴素贝叶斯分类
       * alpha：拉普拉斯平滑系数

6. 朴素贝叶斯算法实例

7. 模型的选择与调优

8. 决策树与随机森林