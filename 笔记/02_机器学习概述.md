# 机器学习概述

## 机器学习概述

### 1. 什么是机器学习

* 机器学习是从**数据**中自动分析获得**规律（模型）**，并利用规律对**未知数据进行预测**
* 示例：AlphaGo、搜索引擎、电商平台个性化推荐

### 2. 为什么要学习机器学习

* 解放生产力
* * 典型代表：智能客服
  * 优势：不知疲倦，进行24小时作业
* 解决专业问题
* * 典型代表：ET医疗
  * 优势：帮助诊断看病
* 提供社会便利
* * 典型代表：杭州 - 城市大脑

### 3. 机器学习的应用场景

* 领域
* * 医疗、航空、教育、物流、电商......
* 目的：
* * 让机器学习程序替换手动的步骤，减少企业的成本也提高企业的效率
* 例子：一个汽车零售商将客户按照对汽车的喜好划分成不同的类，这样营销人员就可以将新型汽车的广告手册直接邮寄到有这种喜好的客户手中，从而大大增加了商业机会



## 数据集的结构

* 数据集的组成
  * 机器学习的数据：文件csv
  * 不用mysql：
    * 1、性能瓶颈，读取速度
    * 2、格式不太符合机器学习要求数据的格式
* 工具：pandas读取工具
  * python有GIL，numpy释放GIL，实现真正的多线程



* 可用的数据集
  * Kaggle：https://www.kaggle.com/datasets
    * 1. 大数据竞赛平台
      2. 80万科学家
      3. 真实数据
      4. 数据量巨大
  * scikit-learn：http://archive.is.uci.edu/ml/
    * 1. 数据量较小
      2. 方便学习
  * UCI：http://scikit-learn.org/stable/datasets/index.html#datasets
    * 1. 收录了360个数据集
      2. 覆盖科学、生活、经济等领域
      3. 数据量几十万
* 数据集结构
  * 特征值+目标值
  * 机器学习中不需要对重复值进行去重处理
* 数据中对于特征的处理
  * pandas：一个数据读取非常方便以及基本的处理格式的工具
  * sklearn：对于**特征的处理**提供了强大的接口

- 对特征值进行处理的整个过程就是特征工程
- 特征工程影响机器学习算法的准确率
- 机器学习过程：
  - 1. 对用户数据进行数据清洗
    2. 机器学习：数据预处理->特征工程->机器学习->模型评估->数据预处理
    3. 离线/在线服务

## 数据的特征工程

* 1. 数据工程是什么

  2. * 特征工程是将**原始数据转换为更好地代表预测模型的潜在问题的特征**的过程，从而**提高了对未知数据的预测准确性** 

  3. 特征工程的意义

     * 直接影响预测结果

  4. scikit-learn库介绍

  5. * Python语言的机器学习**工具**

     * Scikit-learn包括许多知名的**机器学习算法**的实现

     * Scikit-learn文档完善，容易上手，丰富的API，使其在学术界颇受欢迎

     * 安装

       * 创建一个基于Python3的虚拟环境（可以在你自己已有的虚拟环境中）

         mkvirtualenv -p /usr/bin/python3.5 ml3

       * 在ubuntu的虚拟环境中运行以下命令

         pip3 install Scikit-learn

       * 然后通过导入命令查看是否可以使用

         import sklearn

         注：安装scikit-learn需要Numpy, pandas等库

     * api

       * 分类：Classification
       * 回归：Regression
       * 聚类：Clustering
       * 降维：Dimensionality reduction
       * 模型选择：Model selection
       * 特征工程：Preprocessing

  6. 数据的特征抽取

     1. 特征抽取实例演示

        ```python
        # 特征抽取
        # 导入包
        from sklearn.feature_extraction.text import CountVectorizer

        # 实例化CountVectorizer

        vector = CountVectorizer()

        # 调用fit_transform输入并转换数据

        res = vector.fit_transform(["life is short,i like python","life is too long,i dislike python"])

        # 打印结果
        print(vector.get_feature_names())

        print(res.toarray())
        ```

        * 特征抽取对文本等数据进行特征值化
        * 特征值化是为了计算机更好的去理解数据

     2. sklearn特征抽取API

        * sklearn.feature_extraction

     3. 字典特征抽取

        * 作用：对**字典数据**进行特征值化

        * 类：sklearn.feature_extraction.DictVectorizer

        * DictVectorizer语法

          *  DictVectorizer(sparse=True, ...)

          * DictVectorizer.fit_transform(x)

            * x: 字典或者包含**字典的迭代器**
            * 返回值：返回sparse矩阵

          * DictVectorizer.inverse_transform(x)

            * x: array数组或者sparse矩阵

            - 返回值：转换之前数据格式

          * DictVectorizer.get_feature_names()

            * 返回类别名称

          * DictVectorizer.transform(x)

            * 按照原先的标准转换

     4. 文本特征抽取

        - 作用：对**文本数据**进行特征值化
          - 对于单个英文字母、单个汉字不统计（单字母、单个汉字没有分类的依据）
          - 中文需要先进行分词 
        - 类：sklearn.feature_extraction.text.CountVectorizer
        - 应用：文本分类、情感分析
        - CountVectorizer语法
          - CountVectorizer()
            - 返回词频矩阵
            - CountVectorizer.fit_transform(X)
              - X：文本或者包含**文本字符串的可迭代对象**
              - 返回值：返回sparse矩阵
            - CountVectorizer.inverse_transform(X)
              - X：array数组或者sparse矩阵
              - 返回值：转换之前数据格式
            - CountVectorizer.get_feature_names()
              - 返回值：单词列表

     5. tf idf（每个词在文章中的重要程度）

        * 作用：用以评估一字词对于一个文件集或一个词料库中的其中一份文件的**重要程度** ，是分类机器学习算法的重要依据
        * 类：sklearn.feature_extraction.text.TfidfVectorizer
        * 重要性程度：tf * idf


        * tf：term frequency  词的频率
          * 出现的次数
        * idf：inverse document frequency 逆文档频率
          * log(总文档数/该词出现的文档数)
        * TfidfVectorizer语法
        * TfidfVectorizer(stop_words=None,…)
          * 返回词的权重矩阵
        * TfidfVectorizer.fit_transform(X)
          * X：文本或者包含**文本字符串的可迭代对象** 
          * 返回值：返回sparse矩阵
        * TfidfVectorizer.inverse_transform(X)
          * X：array数组或者sparse矩阵
          * 返回值：转换之前数据格式
        * TfidfVectorizer.get_feature_names()
          * 返回值：单词列表

  7. 数据的特征预处理

     * 特征预处理的方法
       * 通过**特定的统计方法（数学方法）**将**数据**转换成**算法要求的数据** 
       * 数据类型
         * 数值型数据：标准缩放：
           1. **归一化**
           2. **标准化**
           3. **缺失值** 
         * 类别型数据：**one-hot编码** 
         * 时间类型：**时间的切分**
     * sklearn中的数据预处理API
       * sklearn.preprocessing

     1. 归一化

        * 特点：通过对原始数据进行变换把数据映射到（默认为[0,1]）之间
        * 公式:
          * $X' = \frac{x-min}{max-min}$
          * $X'' = X' \,(mx-mi) + mi$
          * 注：作用于每一列，max为一列的最大值，min为一列的最小值，那么X''为最终结果，mx,mi分别为指定区间值默认mx为1，mi为0
        * sklearn归一化API：sklearn.preprocessing.MinMaxScaler
        * MinMaxScaler(feature_range=(0,1)...)
          * 每个特征缩放到给定范围（默认[0,1]）
          * MinMaxScaler.fit_transform(X)
            * X：numpy array格式的数据[n_samples, n_features]
            * 返回值：转换后的形状相同的array
        * 目的：使得某一个特征对最终结果不会造成更大影响
        * 问题：如果数据中异常点较多，会有什么影响？
          * 异常点对最大值、最小值影响较大
        * 总结：注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受**异常点**影响，所以这种方法鲁棒性较差，只适合**传统精确小数据场景**

     2. 标准化

        * 特定：通过对原始数据进行变换把数据变换到均值为0，标准差为1范围内
        * 公式：$X' =\frac{ (x-mean) }{a}$
          * 注：作用于每一列，mean为平均值，a为标准差
          * var成为方差：$var =\frac{(x1-mean)^2+(x2-mean)+...}{n(每个特征的样本数)}$
          * $var = a^2$
          * 其中：方差**考量数据的稳定性** 
        * 对于归一化来说：如果出现异常点，影响了**最大值和最小值**，那么结果显然会发生改变
        * 对于标准化来说：如果出现异常点，由于**具有一定数据量，少量的异常点对于平均值的影响并不大**，从而方差改变较小。
        * sklearn标准化API：sklearn.preprocessing.StandardScaler
        * StandardScaler语言
        * StandardScaler(...)
          * 处理之后每列来说所有数据都聚集在**均值0附近标准差为1**
          * StandardScaler.fit_transform(X)
            * X：numpy array格式的数据[n_samples, n_features]
            * 返回值：转换后的形状相同的array
          * StandardScaler.mean_
            * 原始数据中每列特征的平均值
          * StandardScaler.std_
            * 原始数据每列特征的方差
        * 标准化总结：在已有**样本做够多的情况下比较稳定**，适合现代嘈杂大数据场景

     3. 缺失值

        如何处理数据中的缺失值？

        * 删除：如果每列或者行数据缺失值达到一定的比例，建议放弃整行或者整列
        * **插补**：可以通过缺失值**每列**的平均值、中位数来填充 
        * sklearn缺失值API：sklearn.impute.SimpleImputer
        * SimpleImputer语法
        * SimpleImputer(missing_values=np.nan, strategy='mean')
          * 完成缺失值插补
          * SimpleImputer.fit_transform(X)
            * X：numpy array格式的数据[n_samples, n_features]
            * 返回值：转换后的形状相同的array
        * 关于np.nan(np.NaN)
          * numpy的数组中可以使用np.nan/np.NaN来代替缺失值，**属于float类型** 
          * 如果是文件中的一些缺失值，可以替换成nan，通过nap.array转化成float型的数组即可

  8. 数据的降维(特征的数量)

           1. 特征选择
           * 特征选择是什么
             * 特征选择原因
               * 冗余：部分特征的相关度高，容易消耗计算性能
               * 噪声：部分特征对预测结果有影响
             * 特征选择就是单纯的从提取到的**所有特征中选择部分特征**作为训练集特征，特征在**选择前和选择后可以改变值，也可以不改变值**，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征
             * 主要方法（三大武器）：
               * **Filter(过滤式)：VarianceThreshold**
               * **Embedded(嵌入式)：正则化、决策树** 
               * Wrapper(包裹式)
           * sklearn特征选择API
             * sklearn.feature_selection.VarianceThreshold
             * VarianceThreshold语法
             * VarianceThreshold(threshold = 0.0)
               * 删除所有低方差特征
               * Variance.fit_transform(X)
                 * X：numpy array格式的数据[n_samples, n_features]
                 * 返回值：**训练集差异低于threshold的特征将被删除** 
                 * **默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征**
           * 其它特征选择方法
             * 神经网络
           2. 主成分分析
           * sklearn主成分分析API
           * sklearn.decomposition
           * PCA是什么
             * 本质：PCA是一种分析、简化数据集的技术
             * 目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），**损失少量信息** 
             * 作用：**可以削减回归分析或者聚类分析中特征的数量**
               * 当特征数量达到上百的时候，需要考虑数据的简化，数据也会改变，特征数量也会减少 
             * 高纬度数据容易出现的的问题
               * 特征之间通常是**相关的** 
             * 通用公式计算（了解)
               * $Y=PX$即为降维到k维后的数据
               * 矩阵运算得出P为 $P = \left(\begin{matrix}1\sqrt2&1\sqrt2 \\ -1\sqrt2 & 1\sqrt2\end{matrix}\right)$
               * $Y = \begin{matrix}(\frac{1}{\sqrt2} & \frac{1}{\sqrt2})\end{matrix}\left(\begin{matrix}-1 & -1 & 0 & 2 & 0 \\ -2 & 0  & 0 & 1 & 1\end{matrix}\right) = \begin{matrix}(\frac{-3}{\sqrt2} & \frac{-1}{\sqrt2} & 0 & \frac{3}{\sqrt2} & \frac{-1}{\sqrt2})\end{matrix}$
             * PCA语法
             * PCA(n_components=None)
               * 将数据分解为较低维数空间
               * n_components
                 * 小数：保留的信息百分比（0-1），一般是90~95%（经验值）
                 * 整数：减少到的特征数量（一般不会用整数）
               * PCA.fit_transform(X)
                 * X:numpy array格式的数据[n_samples,n_features]
                 * 返回值：转换后**指定维度**的array

## 数据的类型

1. 数值型数据：标准缩放：
   1. **归一化**
   2. **标准化**
   3. **缺失值** 
2. 类别型数据：**one-hot编码** 
3. 时间类型：**时间的切分**

## 机器学习算法基础



